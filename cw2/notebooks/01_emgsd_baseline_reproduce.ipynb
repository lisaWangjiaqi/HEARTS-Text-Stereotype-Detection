{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761c9770",
   "metadata": {},
   "source": [
    "# 1. Environment & Data Loading\n",
    "\n",
    "This notebook reproduces the EMGSD baseline (ALBERT) from the HEARTS repository.\n",
    "We first check the environment and load the EMGSD dataset from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36984aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa/opt/anaconda3/envs/hearts-cw2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.19 (main, Oct 21 2025, 16:37:10) [Clang 20.1.8 ]\n",
      "PyTorch version: 2.5.1\n",
      "Transformers version: 4.46.3\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label'],\n",
      "        num_rows: 45760\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label'],\n",
      "        num_rows: 11441\n",
      "    })\n",
      "})\n",
      "stereotype_type: profession\n",
      "text: The illustrator had tower hands.\n",
      "text_with_marker: The illustrator had ===tower=== hands.\n",
      "category: unrelated\n",
      "data_source: stereoset_intrasentence\n",
      "label: unrelated\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "# Load EMGSD from Hugging Face\n",
    "ds = load_dataset(\"holistic-ai/EMGSD\")\n",
    "print(ds)\n",
    "\n",
    "# Inspect one training example\n",
    "example = ds[\"train\"][0]\n",
    "for k, v in example.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e57970",
   "metadata": {},
   "source": [
    "# 2. Model & Training Configuration\n",
    "\n",
    "We specify the model backbone (ALBERT), the number of labels, and the key training hyperparameters.\n",
    "These values are chosen to be consistent with the original HEARTS repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ec8020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 13\n",
      "Label examples: ['neutral_gender', 'neutral_lgbtq+', 'neutral_nationality', 'neutral_profession', 'neutral_race', 'neutral_religion', 'stereotype_gender', 'stereotype_lgbtq+', 'stereotype_nationality', 'stereotype_profession']\n",
      "Example mapping: [('neutral_gender', 0), ('neutral_lgbtq+', 1), ('neutral_nationality', 2), ('neutral_profession', 3), ('neutral_race', 4)]\n",
      "MODEL_NAME: albert-base-v2\n",
      "NUM_LABELS: 13\n",
      "BATCH_SIZE: 16\n",
      "LEARNING_RATE: 2e-05\n",
      "NUM_EPOCHS: 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Count how many distinct labels we have in EMGSD\n",
    "label_list = sorted(list(set(ds[\"train\"][\"label\"])))\n",
    "print(\"Number of unique labels:\", len(label_list))\n",
    "print(\"Label examples:\", label_list[:10])\n",
    "\n",
    "# Map string labels -> integer ids for training\n",
    "label2id = {lab: i for i, lab in enumerate(label_list)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "print(\"Example mapping:\", list(label2id.items())[:5])\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "NUM_LABELS = len(label_list)\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "print(\"NUM_LABELS:\", NUM_LABELS)\n",
    "print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "print(\"LEARNING_RATE:\", LEARNING_RATE)\n",
    "print(\"NUM_EPOCHS:\", NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f30b2",
   "metadata": {},
   "source": [
    "# 3. Training Script Call\n",
    "\n",
    "We call a separate training script (`cw2/src/train_emgsd_albert.py`) so that\n",
    "the whole baseline can be reproduced from the command line and from this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402fa553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/lisa/opt/anaconda3/envs/hearts-cw2/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[codecarbon INFO @ 22:17:08] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 22:17:08] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 22:17:08] No GPU found.\n",
      "[codecarbon INFO @ 22:17:08] [setup] CPU Tracking...\n",
      "Password:"
     ]
    }
   ],
   "source": [
    "!python ../src/train_emgsd_albert.py \\\n",
    "    --model_name albert-base-v2 \\\n",
    "    --output_dir ../results/emgsd_baseline \\\n",
    "    --batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_epochs 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac220f7",
   "metadata": {},
   "source": [
    "# 4. Evaluation & Comparison\n",
    "\n",
    "After training, we load the saved metrics and compare our EMGSD performance\n",
    "against the values reported in the HEARTS paper (Â±5% tolerance on macro-F1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0eeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_path = Path(\"../results/emgsd_baseline/metrics.json\")\n",
    "\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "    print(\"Loaded metrics:\", metrics)\n",
    "else:\n",
    "    print(\"metrics.json not found yet. Run the training cell above first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hearts-cw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
